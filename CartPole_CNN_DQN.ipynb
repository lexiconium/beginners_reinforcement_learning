{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math, random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "resize = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(40, interpolation=T.functional.InterpolationMode.BICUBIC),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def get_cart_location(screen_w):\n",
    "    world_w = env.x_threshold * 2\n",
    "    scale = screen_w / world_w\n",
    "    return int(screen_w / 2 + scale * env.state[0])\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose(2, 0, 1) # env.render retuns HWC; so transpose it to CHW order\n",
    "    _, screen_h, screen_w = screen.shape\n",
    "\n",
    "    screen = screen[:, int(screen_h * 0.4):int(screen_h * 0.8), :]\n",
    "\n",
    "    view_w = int(screen_w * 0.6)\n",
    "    cart_location = get_cart_location(screen_w)\n",
    "    if cart_location < view_w // 2:\n",
    "        slice_range = slice(view_w)\n",
    "    elif cart_location > (screen_w - view_w // 2):\n",
    "        slice_range = slice(-view_w, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_w // 2, cart_location + view_w // 2)\n",
    "    \n",
    "    screen = screen[:, :, slice_range]\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0) # resize and add a batch dimension; BCHW\n",
    "\n",
    "env.reset()\n",
    "plt.title('Example extracted screen')\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Transition = namedtuple(\n",
    "    typename='Transition',\n",
    "    field_names=('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, k=batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, out_features):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        def conv2d_out_size(size, kernel_size=5, stride=2):\n",
    "            return (size - kernel_size) // stride + 1\n",
    "        conv_h = conv2d_out_size(conv2d_out_size(conv2d_out_size(h)))\n",
    "        conv_w = conv2d_out_size(conv2d_out_size(conv2d_out_size(w)))\n",
    "\n",
    "        self.head = nn.Linear(32 * conv_h * conv_w, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, _, screen_h, screen_w = get_screen().shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_h, screen_w, n_actions).to(device)\n",
    "target_net = DQN(screen_h, screen_w, n_actions).to(device)\n",
    "target_net.load_state_dict(state_dict=policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.98\n",
    "eps_start = 0.1\n",
    "eps_end = 0.01\n",
    "eps_decay = 400\n",
    "target_update = 20\n",
    "\n",
    "def select_action(state, episode):\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1 * episode / eps_decay)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(dim=1)[0].detach()\n",
    "    expected_state_action_values = reward_batch + gamma * next_state_values\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(dim=1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for p in policy_net.parameters():\n",
    "        p.grad.data.clamp_(min=-1, max=1)\n",
    "    optimizer.step()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('episode')\n",
    "    plt.ylabel('duration')\n",
    "\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.plot(durations_t.numpy(), label='score')\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(dimension=0, size=100, step=1).mean(dim=1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100 mean score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_episodes = 10000\n",
    "for epi in range(1, n_episodes + 1):\n",
    "    env.reset()\n",
    "    last_screen, current_screen = get_screen(), get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count(1):\n",
    "        action = select_action(state, epi)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        last_screen, current_screen = current_screen, get_screen()\n",
    "        next_state = None if done else (current_screen - last_screen)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t)\n",
    "            break\n",
    "\n",
    "    for _ in range(10):\n",
    "        optimize_model()\n",
    "    \n",
    "    if epi % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        plot_durations()\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Reference](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}