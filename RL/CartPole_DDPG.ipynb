{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import deque, namedtuple\n",
    "from copy import deepcopy\n",
    "import random, math\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mlp(layer_config, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for i in range(len(layer_config) - 1):\n",
    "        activation = activation if i < len(layer_config) - 2 else output_activation\n",
    "        layers += [nn.Linear(layer_config[i], layer_config[i + 1]), activation()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_layer_config, activation):\n",
    "        super().__init__()\n",
    "        layer_config = [state_dim, *hidden_layer_config, action_dim]\n",
    "        self.policy = mlp(layer_config, activation, output_activation=nn.Tanh)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.policy(state)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_layer_config, activation):\n",
    "        super().__init__()\n",
    "        layer_config = [state_dim + action_dim, *hidden_layer_config, 1]\n",
    "        self.q = mlp(layer_config, activation)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.q(torch.cat([state, action])).squeeze(-1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=1, hidden_layer_config=(128, 128), activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.actor = PolicyNet(state_dim, action_dim, hidden_layer_config, activation)\n",
    "        self.critic = QNet(state_dim, action_dim, hidden_layer_config, activation)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.actor(state)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Transition = namedtuple(\n",
    "    typename='Transtiion',\n",
    "    field_names=('state', 'action', 'reward', 'next_state')\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        def transform(data):\n",
    "            data = torch.tensor(data)\n",
    "            return data if data.ndim > 1 else data.view(-1, 1)\n",
    "        self.buffer.append(Transition(*map(transform, args)))\n",
    "    \n",
    "    def sample(self, batch_size=32):\n",
    "        sample = random.sample(population=self.buffer, k=batch_size)\n",
    "        return Transition(*map(torch.cat, zip(*sample))) # Bind transition fields under batch dimension respectively\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"ddpg.png\" width=\"1006px\" height=\"744px\"></img>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "buffer_capacity = 50000\n",
    "gamma = 0.99\n",
    "lr_q = 1e-3\n",
    "lr_policy = 1e-3\n",
    "polyak = 0.995\n",
    "action_noise = 0.05\n",
    "n_episodes = 10000\n",
    "prelearning_steps = 5000\n",
    "batch_size = 64\n",
    "\n",
    "env_fn = lambda: gym.make('CartPole-v1')\n",
    "\n",
    "# Initialize environment\n",
    "env, test_env = env_fn(), env_fn()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize Actor-Critic networks and corresponding target networks\n",
    "ac = ActorCritic(state_dim=state_dim)\n",
    "target_ac = deepcopy(ac)\n",
    "for p in target_ac.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "def get_q_loss(data: Transition):\n",
    "    state = data.state\n",
    "    action = data.action\n",
    "    reward = data.reward\n",
    "    next_state = data.next_state\n",
    "\n",
    "    q = ac.critic(state, action)\n",
    "    with torch.no_grad():\n",
    "        q_prime = target_ac.critic(next_state, target_ac.actor(next_state))\n",
    "        bellman_backup = reward + (next_state is not None) * gamma * q_prime\n",
    "    \n",
    "    return F.mse_loss(q, bellman_backup)\n",
    "\n",
    "def get_policy_loss(data: Transition):\n",
    "    state, action = data.state, data.action\n",
    "    q = ac.critic(state, action)\n",
    "    return -q.mean() # Lower state-action value has higher loss\n",
    "\n",
    "q_optimizer = optim.Adam(ac.critic.parameters(), lr=lr_q)\n",
    "policy_optimizer = optim.Adam(ac.actor.parameters(), lr=lr_policy)\n",
    "\n",
    "def update(data: Transition):\n",
    "    q_optimizer.zero_grad()\n",
    "    q_loss = get_q_loss(data)\n",
    "    q_loss.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    for p in ac.critic.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss = get_policy_loss(data)\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    for p in ac.critic.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p, target_p in zip(ac.parameters(), target_ac.parameters()):\n",
    "            target_p.data.mul_(polyak)\n",
    "            target_p.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "def get_action(state, noise_scale):\n",
    "    action = ac.actor(state)\n",
    "    action += noise_scale * random.randrange(n_actions)\n",
    "    return torch.clip(action, min=0, max=n_actions - 1).round().long()\n",
    "\n",
    "\n",
    "durations = []\n",
    "for ep in range(1, n_episodes + 1):\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in count(1):\n",
    "        if len(replay_buffer) > prelearning_steps:\n",
    "            action = get_action(state, noise_scale=action_noise)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # replay_buffer.push(state, action, reward, None if done else next_state)\n",
    "        replay_buffer.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > prelearning_steps:\n",
    "            sample = replay_buffer.sample(batch_size=batch_size)\n",
    "            update(sample)\n",
    "\n",
    "        if done:\n",
    "            durations.append(t)\n",
    "            break\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        plt.plot(durations)\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('3.9.4': pyenv)"
  },
  "interpreter": {
   "hash": "f64ca8427f5c7b208fc8e00c8adfc599cb81e9ff0e312b9ab9d7634eae3e3087"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}